{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals, print_function, division\n",
    "import os\n",
    "import glob\n",
    "import struct\n",
    "import csv\n",
    "import queue\n",
    "import time\n",
    "from threading import Thread\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "\n",
    "import logging\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.core.example import example_pb2\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "from torch.optim import Adagrad\n",
    "\n",
    "\n",
    "from rouge import Rouge\n",
    "from pathlib import Path\n",
    "\n",
    "import random\n",
    "from random import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting hyperparameters and spectial token\n",
    "random.seed(123)\n",
    "np.random.seed(123)\n",
    "torch.manual_seed(123)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(123)\n",
    "\n",
    "g_train_data_path = \"data/finished_files/chunked/train_*\"\n",
    "g_eval_data_path = \"data/finished_files/val.bin\"\n",
    "g_decode_data_path = \"data/finished_files/test.bin\"\n",
    "g_vocab_path = \"data/finished_files/vocab\"\n",
    "g_log_root = \"log\"\n",
    "g_model_file_path = None\n",
    "# Hyperparameters\n",
    "g_hidden_dim= 256\n",
    "g_emb_dim= 128\n",
    "g_batch_size= 8\n",
    "g_max_enc_steps=400\n",
    "g_max_dec_steps=100\n",
    "g_beam_size=4\n",
    "g_min_dec_steps=35\n",
    "g_vocab_size=50000\n",
    "g_lr=0.15\n",
    "g_adagrad_init_acc=0.1\n",
    "g_rand_unif_init_mag=0.02\n",
    "g_trunc_norm_init_std=1e-4\n",
    "g_max_grad_norm=2.0\n",
    "g_pointer_gen = True\n",
    "g_is_coverage = True\n",
    "g_cov_loss_wt = 1.0\n",
    "g_eps = 1e-12\n",
    "g_max_iterations = 500000\n",
    "g_use_gpu=True\n",
    "g_lr_coverage=0.15\n",
    "\n",
    "# <s> and </s> are used in the data files to segment the abstracts into sentences. They don't receive vocab ids.\n",
    "SENTENCE_START = '<s>'\n",
    "SENTENCE_END = '</s>'\n",
    "\n",
    "PAD_TOKEN = '[PAD]' # This has a vocab id, which is used to pad the encoder input, decoder input and target sequence\n",
    "UNKNOWN_TOKEN = '[UNK]' # This has a vocab id, which is used to represent out-of-vocabulary words\n",
    "START_DECODING = '[START]' # This has a vocab id, which is used at the start of every decoder input sequence\n",
    "STOP_DECODING = '[STOP]' # This has a vocab id, which is used at the end of untruncated target sequences\n",
    "\n",
    "# Note: none of <s>, </s>, [PAD], [UNK], [START], [STOP] should appear in the vocab file.\n",
    "use_cuda = g_use_gpu and torch.cuda.is_available()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vocabulary, token and id conversion\n",
    "class Vocab(object):\n",
    "\n",
    "  def __init__(self, vocab_file, max_size):\n",
    "    self._word_to_id = {}\n",
    "    self._id_to_word = {}\n",
    "    self._count = 0 # keeps track of total number of words in the Vocab\n",
    "\n",
    "    # [UNK], [PAD], [START] and [STOP] get the ids 0,1,2,3.\n",
    "    for w in [UNKNOWN_TOKEN, PAD_TOKEN, START_DECODING, STOP_DECODING]:\n",
    "      self._word_to_id[w] = self._count\n",
    "      self._id_to_word[self._count] = w\n",
    "      self._count += 1\n",
    "\n",
    "    # Read the vocab file and add words up to max_size\n",
    "    with open(vocab_file, 'r', encoding='utf-8') as vocab_f:\n",
    "      for line in vocab_f:\n",
    "        pieces = line.split()\n",
    "        if len(pieces) != 2:\n",
    "          print('Warning: incorrectly formatted line in vocabulary file: %s\\n' % line)\n",
    "          continue\n",
    "        w = pieces[0]\n",
    "        if w in [SENTENCE_START, SENTENCE_END, UNKNOWN_TOKEN, PAD_TOKEN, START_DECODING, STOP_DECODING]:\n",
    "          raise Exception('<s>, </s>, [UNK], [PAD], [START] and [STOP] shouldn\\'t be in the vocab file, but %s is' % w)\n",
    "        if w in self._word_to_id:\n",
    "          raise Exception('Duplicated word in vocabulary file: %s' % w)\n",
    "        self._word_to_id[w] = self._count\n",
    "        self._id_to_word[self._count] = w\n",
    "        self._count += 1\n",
    "        if max_size != 0 and self._count >= max_size:\n",
    "          print(\"max_size of vocab was specified as %i; we now have %i words. Stopping reading.\" % (max_size, self._count))\n",
    "          break\n",
    "\n",
    "    print(\"Finished constructing vocabulary of %i total words. Last word added: %s\" % (self._count, self._id_to_word[self._count-1]))\n",
    "\n",
    "  def word2id(self, word):\n",
    "    if word not in self._word_to_id:\n",
    "      return self._word_to_id[UNKNOWN_TOKEN]\n",
    "    return self._word_to_id[word]\n",
    "\n",
    "  def id2word(self, word_id):\n",
    "    if word_id not in self._id_to_word:\n",
    "      raise ValueError('Id not found in vocab: %d' % word_id)\n",
    "    return self._id_to_word[word_id]\n",
    "\n",
    "  def size(self):\n",
    "    return self._count\n",
    "\n",
    "  def write_metadata(self, fpath):\n",
    "    print(\"Writing word embedding metadata file to %s...\" % (fpath))\n",
    "    with open(fpath, \"w\") as f:\n",
    "      fieldnames = ['word']\n",
    "      writer = csv.DictWriter(f, delimiter=\"\\t\", fieldnames=fieldnames)\n",
    "      for i in range(self.size()):\n",
    "        writer.writerow({\"word\": self._id_to_word[i]})\n",
    "\n",
    "\n",
    "def example_generator(data_path, single_pass):\n",
    "  while True:\n",
    "    filelist = glob.glob(data_path) # get the list of datafiles\n",
    "    assert filelist, ('Error: Empty filelist at %s' % data_path) # check filelist isn't empty\n",
    "    if single_pass:\n",
    "      filelist = sorted(filelist)\n",
    "    else:\n",
    "      random.shuffle(filelist)\n",
    "    for f in filelist:\n",
    "      reader = open(f, 'rb')\n",
    "      while True:\n",
    "        len_bytes = reader.read(8)\n",
    "        if not len_bytes: break # finished reading this file\n",
    "        str_len = struct.unpack('q', len_bytes)[0]\n",
    "        example_str = struct.unpack('%ds' % str_len, reader.read(str_len))[0]\n",
    "        yield example_pb2.Example.FromString(example_str)\n",
    "    if single_pass:\n",
    "      print(\"example_generator completed reading all datafiles. No more data.\")\n",
    "      break\n",
    "\n",
    "\n",
    "def article2ids(article_words, vocab):\n",
    "  ids = []\n",
    "  oovs = []\n",
    "  unk_id = vocab.word2id(UNKNOWN_TOKEN)\n",
    "  for w in article_words:\n",
    "    i = vocab.word2id(w)\n",
    "    if i == unk_id: # If w is OOV\n",
    "      if w not in oovs: # Add to list of OOVs\n",
    "        oovs.append(w)\n",
    "      oov_num = oovs.index(w) # This is 0 for the first article OOV, 1 for the second article OOV...\n",
    "      ids.append(vocab.size() + oov_num) # This is e.g. 50000 for the first article OOV, 50001 for the second...\n",
    "    else:\n",
    "      ids.append(i)\n",
    "  return ids, oovs\n",
    "\n",
    "\n",
    "def abstract2ids(abstract_words, vocab, article_oovs):\n",
    "  ids = []\n",
    "  unk_id = vocab.word2id(UNKNOWN_TOKEN)\n",
    "  for w in abstract_words:\n",
    "    i = vocab.word2id(w)\n",
    "    if i == unk_id: # If w is an OOV word\n",
    "      if w in article_oovs: # If w is an in-article OOV\n",
    "        vocab_idx = vocab.size() + article_oovs.index(w) # Map to its temporary article OOV number\n",
    "        ids.append(vocab_idx)\n",
    "      else: # If w is an out-of-article OOV\n",
    "        ids.append(unk_id) # Map to the UNK token id\n",
    "    else:\n",
    "      ids.append(i)\n",
    "  return ids\n",
    "\n",
    "\n",
    "def outputids2words(id_list, vocab, article_oovs):\n",
    "  words = []\n",
    "  for i in id_list:\n",
    "    try:\n",
    "      w = vocab.id2word(i) # might be [UNK]\n",
    "    except ValueError as e: # w is OOV\n",
    "      assert article_oovs is not None, \"Error: model produced a word ID that isn't in the vocabulary. This should not happen in baseline (no pointer-generator) mode\"\n",
    "      article_oov_idx = i - vocab.size()\n",
    "      try:\n",
    "        w = article_oovs[article_oov_idx]\n",
    "      except ValueError as e: # i doesn't correspond to an article oov\n",
    "        raise ValueError('Error: model produced word ID %i which corresponds to article OOV %i but this example only has %i article OOVs' % (i, article_oov_idx, len(article_oovs)))\n",
    "    words.append(w)\n",
    "  return words\n",
    "\n",
    "\n",
    "def abstract2sents(abstract):\n",
    "  cur = 0\n",
    "  sents = []\n",
    "  abstract = abstract.decode()\n",
    "  while True:\n",
    "    try:\n",
    "      start_p = abstract.index(SENTENCE_START, cur)\n",
    "      end_p = abstract.index(SENTENCE_END, start_p + 1)\n",
    "      cur = end_p + len(SENTENCE_END)\n",
    "      sents.append(abstract[start_p+len(SENTENCE_START):end_p])\n",
    "    except ValueError as e: # no more sentences\n",
    "      return sents\n",
    "\n",
    "\n",
    "def show_art_oovs(article, vocab):\n",
    "  unk_token = vocab.word2id(UNKNOWN_TOKEN)\n",
    "  words = article.split(' ')\n",
    "  words = [(\"__%s__\" % w) if vocab.word2id(w)==unk_token else w for w in words]\n",
    "  out_str = ' '.join(words)\n",
    "  return out_str\n",
    "\n",
    "\n",
    "def show_abs_oovs(abstract, vocab, article_oovs):\n",
    "  unk_token = vocab.word2id(UNKNOWN_TOKEN)\n",
    "  words = abstract.split(' ')\n",
    "  new_words = []\n",
    "  for w in words:\n",
    "    if vocab.word2id(w) == unk_token: # w is oov\n",
    "      if article_oovs is None: # baseline mode\n",
    "        new_words.append(\"__%s__\" % w)\n",
    "      else: # pointer-generator mode\n",
    "        if w in article_oovs:\n",
    "          new_words.append(\"__%s__\" % w)\n",
    "        else:\n",
    "          new_words.append(\"!!__%s__!!\" % w)\n",
    "    else: # w is in-vocab word\n",
    "      new_words.append(w)\n",
    "  out_str = ' '.join(new_words)\n",
    "  return out_str\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading batch\n",
    "class Example(object):\n",
    "\n",
    "  def __init__(self, article, abstract_sentences, vocab):\n",
    "    # Get ids of special tokens\n",
    "    start_decoding = vocab.word2id(START_DECODING)\n",
    "    stop_decoding = vocab.word2id(STOP_DECODING)\n",
    "\n",
    "    # Process the article\n",
    "    article_words = article.split()\n",
    "    if len(article_words) > g_max_enc_steps:\n",
    "      article_words = article_words[:g_max_enc_steps]\n",
    "    self.enc_len = len(article_words) # store the length after truncation but before padding\n",
    "    self.enc_input = [vocab.word2id(w) for w in article_words] # list of word ids; OOVs are represented by the id for UNK token\n",
    "\n",
    "    # Process the abstract\n",
    "    abstract = ' '.join(abstract_sentences) # string\n",
    "    abstract_words = abstract.split() # list of strings\n",
    "    abs_ids = [vocab.word2id(w) for w in abstract_words] # list of word ids; OOVs are represented by the id for UNK token\n",
    "\n",
    "    # Get the decoder input sequence and target sequence\n",
    "    self.dec_input, self.target = self.get_dec_inp_targ_seqs(abs_ids, g_max_dec_steps, start_decoding, stop_decoding)\n",
    "    self.dec_len = len(self.dec_input)\n",
    "\n",
    "    # If using pointer-generator mode, we need to store some extra info\n",
    "    if g_pointer_gen:\n",
    "      # Store a version of the enc_input where in-article OOVs are represented by their temporary OOV id; also store the in-article OOVs words themselves\n",
    "      self.enc_input_extend_vocab, self.article_oovs = article2ids(article_words, vocab)\n",
    "\n",
    "      # Get a verison of the reference summary where in-article OOVs are represented by their temporary article OOV id\n",
    "      abs_ids_extend_vocab = abstract2ids(abstract_words, vocab, self.article_oovs)\n",
    "\n",
    "      # Overwrite decoder target sequence so it uses the temp article OOV ids\n",
    "      _, self.target = self.get_dec_inp_targ_seqs(abs_ids_extend_vocab, g_max_dec_steps, start_decoding, stop_decoding)\n",
    "\n",
    "    # Store the original strings\n",
    "    self.original_article = article\n",
    "    self.original_abstract = abstract\n",
    "    self.original_abstract_sents = abstract_sentences\n",
    "\n",
    "\n",
    "  def get_dec_inp_targ_seqs(self, sequence, max_len, start_id, stop_id):\n",
    "    inp = [start_id] + sequence[:]\n",
    "    target = sequence[:]\n",
    "    if len(inp) > max_len: # truncate\n",
    "      inp = inp[:max_len]\n",
    "      target = target[:max_len] # no end_token\n",
    "    else: # no truncation\n",
    "      target.append(stop_id) # end token\n",
    "    assert len(inp) == len(target)\n",
    "    return inp, target\n",
    "\n",
    "\n",
    "  def pad_decoder_inp_targ(self, max_len, pad_id):\n",
    "    while len(self.dec_input) < max_len:\n",
    "      self.dec_input.append(pad_id)\n",
    "    while len(self.target) < max_len:\n",
    "      self.target.append(pad_id)\n",
    "\n",
    "\n",
    "  def pad_encoder_input(self, max_len, pad_id):\n",
    "    while len(self.enc_input) < max_len:\n",
    "      self.enc_input.append(pad_id)\n",
    "    if g_pointer_gen:\n",
    "      while len(self.enc_input_extend_vocab) < max_len:\n",
    "        self.enc_input_extend_vocab.append(pad_id)\n",
    "\n",
    "\n",
    "class Batch(object):\n",
    "  def __init__(self, example_list, vocab, batch_size):\n",
    "    self.batch_size = batch_size\n",
    "    self.pad_id = vocab.word2id(PAD_TOKEN) # id of the PAD token used to pad sequences\n",
    "    self.init_encoder_seq(example_list) # initialize the input to the encoder\n",
    "    self.init_decoder_seq(example_list) # initialize the input and targets for the decoder\n",
    "    self.store_orig_strings(example_list) # store the original strings\n",
    "\n",
    "\n",
    "  def init_encoder_seq(self, example_list):\n",
    "    # Determine the maximum length of the encoder input sequence in this batch\n",
    "    max_enc_seq_len = max([ex.enc_len for ex in example_list])\n",
    "\n",
    "    # Pad the encoder input sequences up to the length of the longest sequence\n",
    "    for ex in example_list:\n",
    "      ex.pad_encoder_input(max_enc_seq_len, self.pad_id)\n",
    "\n",
    "    # Initialize the numpy arrays\n",
    "    # Note: our enc_batch can have different length (second dimension) for each batch because we use dynamic_rnn for the encoder.\n",
    "    self.enc_batch = np.zeros((self.batch_size, max_enc_seq_len), dtype=np.int32)\n",
    "    self.enc_lens = np.zeros((self.batch_size), dtype=np.int32)\n",
    "    self.enc_padding_mask = np.zeros((self.batch_size, max_enc_seq_len), dtype=np.float32)\n",
    "\n",
    "    # Fill in the numpy arrays\n",
    "    for i, ex in enumerate(example_list):\n",
    "      self.enc_batch[i, :] = ex.enc_input[:]\n",
    "      self.enc_lens[i] = ex.enc_len\n",
    "      for j in range(ex.enc_len):\n",
    "        self.enc_padding_mask[i][j] = 1\n",
    "\n",
    "    # For pointer-generator mode, need to store some extra info\n",
    "    if g_pointer_gen:\n",
    "      # Determine the max number of in-article OOVs in this batch\n",
    "      self.max_art_oovs = max([len(ex.article_oovs) for ex in example_list])\n",
    "      # Store the in-article OOVs themselves\n",
    "      self.art_oovs = [ex.article_oovs for ex in example_list]\n",
    "      # Store the version of the enc_batch that uses the article OOV ids\n",
    "      self.enc_batch_extend_vocab = np.zeros((self.batch_size, max_enc_seq_len), dtype=np.int32)\n",
    "      for i, ex in enumerate(example_list):\n",
    "        self.enc_batch_extend_vocab[i, :] = ex.enc_input_extend_vocab[:]\n",
    "\n",
    "  def init_decoder_seq(self, example_list):\n",
    "    # Pad the inputs and targets\n",
    "    for ex in example_list:\n",
    "      ex.pad_decoder_inp_targ(g_max_dec_steps, self.pad_id)\n",
    "\n",
    "    # Initialize the numpy arrays.\n",
    "    self.dec_batch = np.zeros((self.batch_size, g_max_dec_steps), dtype=np.int32)\n",
    "    self.target_batch = np.zeros((self.batch_size, g_max_dec_steps), dtype=np.int32)\n",
    "    self.dec_padding_mask = np.zeros((self.batch_size, g_max_dec_steps), dtype=np.float32)\n",
    "    self.dec_lens = np.zeros((self.batch_size), dtype=np.int32)\n",
    "\n",
    "    # Fill in the numpy arrays\n",
    "    for i, ex in enumerate(example_list):\n",
    "      self.dec_batch[i, :] = ex.dec_input[:]\n",
    "      self.target_batch[i, :] = ex.target[:]\n",
    "      self.dec_lens[i] = ex.dec_len\n",
    "      for j in range(ex.dec_len):\n",
    "        self.dec_padding_mask[i][j] = 1\n",
    "\n",
    "  def store_orig_strings(self, example_list):\n",
    "    self.original_articles = [ex.original_article for ex in example_list] # list of lists\n",
    "    self.original_abstracts = [ex.original_abstract for ex in example_list] # list of lists\n",
    "    self.original_abstracts_sents = [ex.original_abstract_sents for ex in example_list] # list of list of lists\n",
    "\n",
    "\n",
    "class Batcher(object):\n",
    "  BATCH_QUEUE_MAX = 100 # max number of batches the batch_queue can hold\n",
    "\n",
    "  def __init__(self, data_path, vocab, mode, batch_size, single_pass):\n",
    "    self._data_path = data_path\n",
    "    self._vocab = vocab\n",
    "    self._single_pass = single_pass\n",
    "    self.mode = mode\n",
    "    self.batch_size = batch_size\n",
    "    # Initialize a queue of Batches waiting to be used, and a queue of Examples waiting to be batched\n",
    "    self._batch_queue = queue.Queue(self.BATCH_QUEUE_MAX)\n",
    "    self._example_queue = queue.Queue(self.BATCH_QUEUE_MAX * self.batch_size)\n",
    "\n",
    "    # Different settings depending on whether we're in single_pass mode or not\n",
    "    if single_pass:\n",
    "      self._num_example_q_threads = 1 # just one thread, so we read through the dataset just once\n",
    "      self._num_batch_q_threads = 1  # just one thread to batch examples\n",
    "      self._bucketing_cache_size = 1 # only load one batch's worth of examples before bucketing; this essentially means no bucketing\n",
    "      self._finished_reading = False # this will tell us when we're finished reading the dataset\n",
    "    else:\n",
    "      self._num_example_q_threads = 1 #16 # num threads to fill example queue\n",
    "      self._num_batch_q_threads = 1 #4  # num threads to fill batch queue\n",
    "      self._bucketing_cache_size = 1 #100 # how many batches-worth of examples to load into cache before bucketing\n",
    "\n",
    "    # Start the threads that load the queues\n",
    "    self._example_q_threads = []\n",
    "    for _ in range(self._num_example_q_threads):\n",
    "      self._example_q_threads.append(Thread(target=self.fill_example_queue))\n",
    "      self._example_q_threads[-1].daemon = True\n",
    "      self._example_q_threads[-1].start()\n",
    "    self._batch_q_threads = []\n",
    "    for _ in range(self._num_batch_q_threads):\n",
    "      self._batch_q_threads.append(Thread(target=self.fill_batch_queue))\n",
    "      self._batch_q_threads[-1].daemon = True\n",
    "      self._batch_q_threads[-1].start()\n",
    "\n",
    "    # Start a thread that watches the other threads and restarts them if they're dead\n",
    "    if not single_pass: # We don't want a watcher in single_pass mode because the threads shouldn't run forever\n",
    "      self._watch_thread = Thread(target=self.watch_threads)\n",
    "      self._watch_thread.daemon = True\n",
    "      self._watch_thread.start()\n",
    "\n",
    "  def next_batch(self):\n",
    "    # If the batch queue is empty, print a warning\n",
    "    if self._batch_queue.qsize() == 0:\n",
    "      tf.compat.v1.logging.warning('Bucket input queue is empty when calling next_batch. Bucket queue size: %i, Input queue size: %i', self._batch_queue.qsize(), self._example_queue.qsize())\n",
    "      if self._single_pass and self._finished_reading:\n",
    "        tf.compat.v1.logging.info(\"Finished reading dataset in single_pass mode.\")\n",
    "        return None\n",
    "\n",
    "    batch = self._batch_queue.get() # get the next Batch\n",
    "    return batch\n",
    "\n",
    "  def fill_example_queue(self):\n",
    "    input_gen = self.text_generator(example_generator(self._data_path, self._single_pass))\n",
    "\n",
    "    while True:\n",
    "      try:\n",
    "        (article, abstract) = input_gen.__next__() # read the next example from file. article and abstract are both strings.\n",
    "      except StopIteration: # if there are no more examples:\n",
    "        tf.compat.v1.logging.info(\"The example generator for this example queue filling thread has exhausted \")\n",
    "        if self._single_pass:\n",
    "          tf.compat.v1.logging.info(\"single_pass mode is on, so we've finished reading dataset. This thread is stopping.\")\n",
    "          self._finished_reading = True\n",
    "          break\n",
    "        else:\n",
    "          raise Exception(\"single_pass mode is off but the example generator is out of data; error.\")\n",
    "\n",
    "      abstract_sentences = [sent.strip() for sent in abstract2sents(abstract)] # Use the <s> and </s> tags in abstract to get a list of sentences.\n",
    "      example = Example(article, abstract_sentences, self._vocab) # Process into an Example.\n",
    "      self._example_queue.put(example) # place the Example in the example queue.\n",
    "\n",
    "  def fill_batch_queue(self):\n",
    "    while True:\n",
    "      if self.mode == 'decode':\n",
    "        # beam search decode mode single example repeated in the batch\n",
    "        ex = self._example_queue.get()\n",
    "        b = [ex for _ in range(self.batch_size)]\n",
    "        self._batch_queue.put(Batch(b, self._vocab, self.batch_size))\n",
    "      else:\n",
    "        # Get bucketing_cache_size-many batches of Examples into a list, then sort\n",
    "        inputs = []\n",
    "        for _ in range(self.batch_size * self._bucketing_cache_size):\n",
    "          inputs.append(self._example_queue.get())\n",
    "        inputs = sorted(inputs, key=lambda inp: inp.enc_len, reverse=True) # sort by length of encoder sequence\n",
    "\n",
    "        # Group the sorted Examples into batches, optionally shuffle the batches, and place in the batch queue.\n",
    "        batches = []\n",
    "        for i in range(0, len(inputs), self.batch_size):\n",
    "          batches.append(inputs[i:i + self.batch_size])\n",
    "        if not self._single_pass:\n",
    "          shuffle(batches)\n",
    "        for b in batches:  # each b is a list of Example objects\n",
    "          self._batch_queue.put(Batch(b, self._vocab, self.batch_size))\n",
    "\n",
    "  def watch_threads(self):\n",
    "    while True:\n",
    "      tf.compat.v1.logging.info(\n",
    "        'Bucket queue size: %i, Input queue size: %i',\n",
    "        self._batch_queue.qsize(), self._example_queue.qsize())\n",
    "\n",
    "      time.sleep(60)\n",
    "      for idx,t in enumerate(self._example_q_threads):\n",
    "        if not t.is_alive(): # if the thread is dead\n",
    "          tf.compat.v1.logging.error('Found example queue thread dead. Restarting.')\n",
    "          new_t = Thread(target=self.fill_example_queue)\n",
    "          self._example_q_threads[idx] = new_t\n",
    "          new_t.daemon = True\n",
    "          new_t.start()\n",
    "      for idx,t in enumerate(self._batch_q_threads):\n",
    "        if not t.is_alive(): # if the thread is dead\n",
    "          tf.compat.v1.logging.error('Found batch queue thread dead. Restarting.')\n",
    "          new_t = Thread(target=self.fill_batch_queue)\n",
    "          self._batch_q_threads[idx] = new_t\n",
    "          new_t.daemon = True\n",
    "          new_t.start()\n",
    "\n",
    "\n",
    "  def text_generator(self, example_generator):\n",
    "    while True:\n",
    "      e = example_generator.__next__() # e is a tf.Example\n",
    "      try:\n",
    "        article_text = e.features.feature['article'].bytes_list.value[0] # the article text was saved under the key 'article' in the data files\n",
    "        abstract_text = e.features.feature['abstract'].bytes_list.value[0] # the abstract text was saved under the key 'abstract' in the data files\n",
    "      except ValueError:\n",
    "        tf.compat.v1.logging.error('Failed to get article or abstract from example')\n",
    "        continue\n",
    "      if len(article_text)==0: # See https://github.com/abisee/pointer-generator/issues/1\n",
    "        #tf.logging.warning('Found an example with empty article text. Skipping it.')\n",
    "        continue\n",
    "      else:\n",
    "        yield (article_text, abstract_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building lstm model\n",
    "def init_lstm_wt(lstm):\n",
    "    for names in lstm._all_weights:\n",
    "        for name in names:\n",
    "            if name.startswith('weight_'):\n",
    "                wt = getattr(lstm, name)\n",
    "                wt.data.uniform_(-g_rand_unif_init_mag, g_rand_unif_init_mag)\n",
    "            elif name.startswith('bias_'):\n",
    "                # set forget bias to 1\n",
    "                bias = getattr(lstm, name)\n",
    "                n = bias.size(0)\n",
    "                start, end = n // 4, n // 2\n",
    "                bias.data.fill_(0.)\n",
    "                bias.data[start:end].fill_(1.)\n",
    "\n",
    "def init_linear_wt(linear):\n",
    "    linear.weight.data.normal_(std=g_trunc_norm_init_std)\n",
    "    if linear.bias is not None:\n",
    "        linear.bias.data.normal_(std=g_trunc_norm_init_std)\n",
    "\n",
    "def init_wt_normal(wt):\n",
    "    wt.data.normal_(std=g_trunc_norm_init_std)\n",
    "\n",
    "def init_wt_unif(wt):\n",
    "    wt.data.uniform_(-g_rand_unif_init_mag, g_rand_unif_init_mag)\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.embedding = nn.Embedding(g_vocab_size, g_emb_dim)\n",
    "        init_wt_normal(self.embedding.weight)\n",
    "        \n",
    "        self.lstm = nn.LSTM(g_emb_dim, g_hidden_dim, num_layers=1, batch_first=True, bidirectional=True)\n",
    "        init_lstm_wt(self.lstm)\n",
    "\n",
    "        self.W_h = nn.Linear(g_hidden_dim * 2, g_hidden_dim * 2, bias=False)\n",
    "\n",
    "    #seq_lens should be in descending order\n",
    "    def forward(self, input, seq_lens):\n",
    "        embedded = self.embedding(input)\n",
    "       \n",
    "        packed = pack_padded_sequence(embedded, seq_lens, batch_first=True)\n",
    "        output, hidden = self.lstm(packed)\n",
    "\n",
    "        encoder_outputs, _ = pad_packed_sequence(output, batch_first=True)  # h dim = B x t_k x n\n",
    "        encoder_outputs = encoder_outputs.contiguous()\n",
    "        \n",
    "        encoder_feature = encoder_outputs.view(-1, 2*g_hidden_dim)  # B * t_k x 2*hidden_dim\n",
    "        encoder_feature = self.W_h(encoder_feature)\n",
    "\n",
    "        return encoder_outputs, encoder_feature, hidden\n",
    "\n",
    "class ReduceState(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ReduceState, self).__init__()\n",
    "\n",
    "        self.reduce_h = nn.Linear(g_hidden_dim * 2, g_hidden_dim)\n",
    "        init_linear_wt(self.reduce_h)\n",
    "        self.reduce_c = nn.Linear(g_hidden_dim * 2, g_hidden_dim)\n",
    "        init_linear_wt(self.reduce_c)\n",
    "\n",
    "    def forward(self, hidden):\n",
    "        h, c = hidden # h, c dim = 2 x b x hidden_dim\n",
    "        h_in = h.transpose(0, 1).contiguous().view(-1, g_hidden_dim * 2)\n",
    "        hidden_reduced_h = F.relu(self.reduce_h(h_in))\n",
    "        c_in = c.transpose(0, 1).contiguous().view(-1, g_hidden_dim * 2)\n",
    "        hidden_reduced_c = F.relu(self.reduce_c(c_in))\n",
    "\n",
    "        return (hidden_reduced_h.unsqueeze(0), hidden_reduced_c.unsqueeze(0)) # h, c dim = 1 x b x hidden_dim\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Attention, self).__init__()\n",
    "        # attention\n",
    "        if g_is_coverage:\n",
    "            self.W_c = nn.Linear(1, g_hidden_dim * 2, bias=False)\n",
    "        self.decode_proj = nn.Linear(g_hidden_dim * 2, g_hidden_dim * 2)\n",
    "        self.v = nn.Linear(g_hidden_dim * 2, 1, bias=False)\n",
    "\n",
    "    def forward(self, s_t_hat, encoder_outputs, encoder_feature, enc_padding_mask, coverage):\n",
    "        b, t_k, n = list(encoder_outputs.size())\n",
    "\n",
    "        dec_fea = self.decode_proj(s_t_hat) # B x 2*hidden_dim\n",
    "        dec_fea_expanded = dec_fea.unsqueeze(1).expand(b, t_k, n).contiguous() # B x t_k x 2*hidden_dim\n",
    "        dec_fea_expanded = dec_fea_expanded.view(-1, n)  # B * t_k x 2*hidden_dim\n",
    "\n",
    "        att_features = encoder_feature + dec_fea_expanded # B * t_k x 2*hidden_dim\n",
    "        if g_is_coverage:\n",
    "            coverage_input = coverage.view(-1, 1)  # B * t_k x 1\n",
    "            coverage_feature = self.W_c(coverage_input)  # B * t_k x 2*hidden_dim\n",
    "            att_features = att_features + coverage_feature\n",
    "\n",
    "        e = torch.tanh(att_features) # B * t_k x 2*hidden_dim\n",
    "        scores = self.v(e)  # B * t_k x 1\n",
    "        scores = scores.view(-1, t_k)  # B x t_k\n",
    "\n",
    "        attn_dist_ = torch.softmax(scores, dim=1)*enc_padding_mask # B x t_k\n",
    "        normalization_factor = attn_dist_.sum(1, keepdim=True)\n",
    "        attn_dist = attn_dist_ / normalization_factor\n",
    "\n",
    "        attn_dist = attn_dist.unsqueeze(1)  # B x 1 x t_k\n",
    "        c_t = torch.bmm(attn_dist, encoder_outputs)  # B x 1 x n\n",
    "        c_t = c_t.view(-1, g_hidden_dim * 2)  # B x 2*hidden_dim\n",
    "\n",
    "        attn_dist = attn_dist.view(-1, t_k)  # B x t_k\n",
    "\n",
    "        if g_is_coverage:\n",
    "            coverage = coverage.view(-1, t_k)\n",
    "            coverage = coverage + attn_dist\n",
    "\n",
    "        return c_t, attn_dist, coverage\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.attention_network = Attention()\n",
    "        # decoder\n",
    "        self.embedding = nn.Embedding(g_vocab_size, g_emb_dim)\n",
    "        init_wt_normal(self.embedding.weight)\n",
    "\n",
    "        self.x_context = nn.Linear(g_hidden_dim * 2 + g_emb_dim, g_emb_dim)\n",
    "\n",
    "        self.lstm = nn.LSTM(g_emb_dim, g_hidden_dim, num_layers=1, batch_first=True, bidirectional=False)\n",
    "        init_lstm_wt(self.lstm)\n",
    "\n",
    "        if g_pointer_gen:\n",
    "            self.p_gen_linear = nn.Linear(g_hidden_dim * 4 + g_emb_dim, 1)\n",
    "\n",
    "        #p_vocab\n",
    "        self.out1 = nn.Linear(g_hidden_dim * 3, g_hidden_dim)\n",
    "        self.out2 = nn.Linear(g_hidden_dim, g_vocab_size)\n",
    "        init_linear_wt(self.out2)\n",
    "\n",
    "    def forward(self, y_t_1, s_t_1, encoder_outputs, encoder_feature, enc_padding_mask,\n",
    "                c_t_1, extra_zeros, enc_batch_extend_vocab, coverage, step):\n",
    "\n",
    "        if not self.training and step == 0:\n",
    "            h_decoder, c_decoder = s_t_1\n",
    "            s_t_hat = torch.cat((h_decoder.view(-1, g_hidden_dim),\n",
    "                                 c_decoder.view(-1, g_hidden_dim)), 1)  # B x 2*hidden_dim\n",
    "            c_t, _, coverage_next = self.attention_network(s_t_hat, encoder_outputs, encoder_feature,\n",
    "                                                              enc_padding_mask, coverage)\n",
    "            coverage = coverage_next\n",
    "\n",
    "        y_t_1_embd = self.embedding(y_t_1)\n",
    "        x = self.x_context(torch.cat((c_t_1, y_t_1_embd), 1))\n",
    "        lstm_out, s_t = self.lstm(x.unsqueeze(1), s_t_1)\n",
    "\n",
    "        h_decoder, c_decoder = s_t\n",
    "        s_t_hat = torch.cat((h_decoder.view(-1, g_hidden_dim),\n",
    "                             c_decoder.view(-1, g_hidden_dim)), 1)  # B x 2*hidden_dim\n",
    "        c_t, attn_dist, coverage_next = self.attention_network(s_t_hat, encoder_outputs, encoder_feature,\n",
    "                                                          enc_padding_mask, coverage)\n",
    "\n",
    "        if self.training or step > 0:\n",
    "            coverage = coverage_next\n",
    "\n",
    "        p_gen = None\n",
    "        if g_pointer_gen:\n",
    "            p_gen_input = torch.cat((c_t, s_t_hat, x), 1)  # B x (2*2*hidden_dim + emb_dim)\n",
    "            p_gen = self.p_gen_linear(p_gen_input)\n",
    "            p_gen = torch.sigmoid(p_gen)\n",
    "\n",
    "        output = torch.cat((lstm_out.view(-1, g_hidden_dim), c_t), 1) # B x hidden_dim * 3\n",
    "        output = self.out1(output) # B x hidden_dim\n",
    "\n",
    "        #output = F.relu(output)\n",
    "\n",
    "        output = self.out2(output) # B x vocab_size\n",
    "        vocab_dist = torch.softmax(output, dim=1)\n",
    "\n",
    "        if g_pointer_gen:\n",
    "            vocab_dist_ = p_gen * vocab_dist\n",
    "            attn_dist_ = (1 - p_gen) * attn_dist\n",
    "\n",
    "            if extra_zeros is not None:\n",
    "                vocab_dist_ = torch.cat([vocab_dist_, extra_zeros], 1)\n",
    "\n",
    "            final_dist = vocab_dist_.scatter_add(1, enc_batch_extend_vocab, attn_dist_)\n",
    "        else:\n",
    "            final_dist = vocab_dist\n",
    "\n",
    "        return final_dist, s_t, c_t, attn_dist, p_gen, coverage\n",
    "\n",
    "class Model(object):\n",
    "    def __init__(self, model_file_path=None, is_eval=False):\n",
    "        encoder = Encoder()\n",
    "        decoder = Decoder()\n",
    "        reduce_state = ReduceState()\n",
    "\n",
    "        # shared the embedding between encoder and decoder\n",
    "        decoder.embedding.weight = encoder.embedding.weight\n",
    "        if is_eval:\n",
    "            encoder = encoder.eval()\n",
    "            decoder = decoder.eval()\n",
    "            reduce_state = reduce_state.eval()\n",
    "\n",
    "        if use_cuda:\n",
    "            encoder = encoder.cuda()\n",
    "            decoder = decoder.cuda()\n",
    "            reduce_state = reduce_state.cuda()\n",
    "\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.reduce_state = reduce_state\n",
    "\n",
    "        if model_file_path is not None:\n",
    "            state = torch.load(model_file_path, map_location= lambda storage, location: storage)\n",
    "            self.encoder.load_state_dict(state['encoder_state_dict'])\n",
    "            self.decoder.load_state_dict(state['decoder_state_dict'], strict=False)\n",
    "            self.reduce_state.load_state_dict(state['reduce_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the tools used in training the test set\n",
    "def get_input_from_batch(batch, use_cuda):\n",
    "  batch_size = len(batch.enc_lens)\n",
    "\n",
    "  enc_batch = torch.from_numpy(batch.enc_batch).long()\n",
    "  enc_padding_mask = torch.from_numpy(batch.enc_padding_mask).float()\n",
    "  enc_lens = batch.enc_lens\n",
    "  extra_zeros = None\n",
    "  enc_batch_extend_vocab = None\n",
    "\n",
    "  if g_pointer_gen:\n",
    "    enc_batch_extend_vocab = torch.from_numpy(batch.enc_batch_extend_vocab).long()\n",
    "    # max_art_oovs is the max over all the article oov list in the batch\n",
    "    if batch.max_art_oovs > 0:\n",
    "      extra_zeros = torch.zeros((batch_size, batch.max_art_oovs))\n",
    "\n",
    "  c_t_1 = torch.zeros((batch_size, 2 * g_hidden_dim))\n",
    "\n",
    "  coverage = None\n",
    "  if g_is_coverage:\n",
    "    coverage = torch.zeros(enc_batch.size())\n",
    "\n",
    "  if use_cuda:\n",
    "    enc_batch = enc_batch.cuda()\n",
    "    enc_padding_mask = enc_padding_mask.cuda()\n",
    "\n",
    "    if enc_batch_extend_vocab is not None:\n",
    "      enc_batch_extend_vocab = enc_batch_extend_vocab.cuda()\n",
    "    if extra_zeros is not None:\n",
    "      extra_zeros = extra_zeros.cuda()\n",
    "    c_t_1 = c_t_1.cuda()\n",
    "\n",
    "    if coverage is not None:\n",
    "      coverage = coverage.cuda()\n",
    "\n",
    "  return enc_batch, enc_padding_mask, enc_lens, enc_batch_extend_vocab, extra_zeros, c_t_1, coverage\n",
    "\n",
    "def get_output_from_batch(batch, use_cuda):\n",
    "  dec_batch = torch.from_numpy(batch.dec_batch).long()\n",
    "  dec_padding_mask = torch.from_numpy(batch.dec_padding_mask).float()\n",
    "  dec_lens = batch.dec_lens\n",
    "  max_dec_len = np.max(dec_lens)\n",
    "  dec_lens_var = torch.from_numpy(dec_lens).float()\n",
    "\n",
    "  target_batch = torch.from_numpy(batch.target_batch).long()\n",
    "\n",
    "  if use_cuda:\n",
    "    dec_batch = dec_batch.cuda()\n",
    "    dec_padding_mask = dec_padding_mask.cuda()\n",
    "    dec_lens_var = dec_lens_var.cuda()\n",
    "    target_batch = target_batch.cuda()\n",
    "\n",
    "\n",
    "  return dec_batch, dec_padding_mask, max_dec_len, dec_lens_var, target_batch\n",
    "\n",
    "# utils\n",
    "def print_results(article, abstract, decoded_output):\n",
    "  print (\"\")\n",
    "  print('ARTICLE:  %s', article)\n",
    "  print('REFERENCE SUMMARY: %s', abstract)\n",
    "  print('GENERATED SUMMARY: %s', decoded_output)\n",
    "  print( \"\")\n",
    "\n",
    "\n",
    "def make_html_safe(s):\n",
    "  s.replace(\"<\", \"&lt;\")\n",
    "  s.replace(\">\", \"&gt;\")\n",
    "  return s\n",
    "\n",
    "\n",
    "def calc_running_avg_loss(loss, running_avg_loss, summary_writer, step, decay=0.99):\n",
    "  if running_avg_loss == 0:  # on the first iteration just take the loss\n",
    "    running_avg_loss = loss\n",
    "  else:\n",
    "    running_avg_loss = running_avg_loss * decay + (1 - decay) * loss\n",
    "  running_avg_loss = min(running_avg_loss, 12)  # clip\n",
    "  \n",
    " \n",
    "  tag_name = 'running_avg_loss/decay=%f' % (decay)\n",
    "  # loss_sum = tf.compat.v1.Summary(value=[tf.compat.v1.Summary.Value(tag=tag_name, simple_value=running_avg_loss)])\n",
    "  # loss_sum.value.add(tag=tag_name, simple_value=running_avg_loss)\n",
    "  summary_writer.add_scalar(tag_name, running_avg_loss, step)\n",
    "  return running_avg_loss\n",
    "\n",
    "def write_for_rouge(reference_sents, decoded_words, ex_index,\n",
    "                    _rouge_ref_dir, _rouge_dec_dir):\n",
    "  decoded_sents = []\n",
    "  while len(decoded_words) > 0:\n",
    "    try:\n",
    "      fst_period_idx = decoded_words.index(\".\")\n",
    "    except ValueError:\n",
    "      fst_period_idx = len(decoded_words)\n",
    "    sent = decoded_words[:fst_period_idx + 1]\n",
    "    decoded_words = decoded_words[fst_period_idx + 1:]\n",
    "    decoded_sents.append(' '.join(sent))\n",
    "\n",
    "  # pyrouge calls a perl script that puts the data into HTML files.\n",
    "  # Therefore we need to make our output HTML safe.\n",
    "  decoded_sents = [make_html_safe(w) for w in decoded_sents]\n",
    "  reference_sents = [make_html_safe(w) for w in reference_sents]\n",
    "\n",
    "  ref_file = os.path.join(_rouge_ref_dir, \"%06d_reference.txt\" % ex_index)\n",
    "  decoded_file = os.path.join(_rouge_dec_dir, \"%06d_decoded.txt\" % ex_index)\n",
    "\n",
    "  with open(ref_file, \"w\") as f:\n",
    "    for idx, sent in enumerate(reference_sents):\n",
    "      f.write(sent) if idx == len(reference_sents) - 1 else f.write(sent + \"\\n\")\n",
    "  with open(decoded_file, \"w\") as f:\n",
    "    for idx, sent in enumerate(decoded_sents):\n",
    "      f.write(sent) if idx == len(decoded_sents) - 1 else f.write(sent + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the process of training \n",
    "class Train(object):\n",
    "    def __init__(self):\n",
    "        self.vocab = Vocab(g_vocab_path, g_vocab_size)\n",
    "        self.batcher = Batcher(g_train_data_path, self.vocab, mode='train',\n",
    "                               batch_size=g_batch_size, single_pass=False)\n",
    "        time.sleep(15)\n",
    "\n",
    "        train_dir = os.path.join(g_log_root, 'train_%d' % (int(time.time())))\n",
    "        if not os.path.exists(train_dir):\n",
    "            os.mkdir(train_dir)\n",
    "\n",
    "        self.model_dir = os.path.join(train_dir, 'model')\n",
    "        if not os.path.exists(self.model_dir):\n",
    "            os.mkdir(self.model_dir)\n",
    "\n",
    "        self.summary_writer = SummaryWriter(train_dir)\n",
    "\n",
    "    def save_model(self, running_avg_loss, iter):\n",
    "        state = {\n",
    "            'iter': iter,\n",
    "            'encoder_state_dict': self.model.encoder.state_dict(),\n",
    "            'decoder_state_dict': self.model.decoder.state_dict(),\n",
    "            'reduce_state_dict': self.model.reduce_state.state_dict(),\n",
    "            'optimizer': self.optimizer.state_dict(),\n",
    "            'current_loss': running_avg_loss\n",
    "        }\n",
    "        model_save_path = os.path.join(self.model_dir, 'model_%d_%d' % (iter, int(time.time())))\n",
    "        torch.save(state, model_save_path)\n",
    "\n",
    "    def setup_train(self, model_file_path=None):\n",
    "        self.model = Model(model_file_path)\n",
    "\n",
    "        params = list(self.model.encoder.parameters()) + list(self.model.decoder.parameters()) + \\\n",
    "                 list(self.model.reduce_state.parameters())\n",
    "        initial_lr = g_lr_coverage if g_is_coverage else g_lr\n",
    "        self.optimizer = Adagrad(params, lr=initial_lr, initial_accumulator_value=g_adagrad_init_acc)\n",
    "\n",
    "        start_iter, start_loss = 0, 0\n",
    "\n",
    "        if model_file_path is not None:\n",
    "            state = torch.load(model_file_path, map_location= lambda storage, location: storage)\n",
    "            start_iter = state['iter']\n",
    "            start_loss = state['current_loss']\n",
    "\n",
    "            if not g_is_coverage:\n",
    "                self.optimizer.load_state_dict(state['optimizer'])\n",
    "                if use_cuda:\n",
    "                    for state in self.optimizer.state.values():\n",
    "                        for k, v in state.items():\n",
    "                            if torch.is_tensor(v):\n",
    "                                state[k] = v.cuda()\n",
    "\n",
    "        return start_iter, start_loss\n",
    "\n",
    "    def train_one_batch(self, batch):\n",
    "        enc_batch, enc_padding_mask, enc_lens, enc_batch_extend_vocab, extra_zeros, c_t_1, coverage = \\\n",
    "            get_input_from_batch(batch, use_cuda)\n",
    "        dec_batch, dec_padding_mask, max_dec_len, dec_lens_var, target_batch = \\\n",
    "            get_output_from_batch(batch, use_cuda)\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "\n",
    "        encoder_outputs, encoder_feature, encoder_hidden = self.model.encoder(enc_batch, enc_lens)\n",
    "        s_t_1 = self.model.reduce_state(encoder_hidden)\n",
    "\n",
    "        step_losses = []\n",
    "        for di in range(min(max_dec_len, g_max_dec_steps)):\n",
    "            y_t_1 = dec_batch[:, di]  # Teacher forcing\n",
    "            final_dist, s_t_1,  c_t_1, attn_dist, p_gen, next_coverage = self.model.decoder(y_t_1, s_t_1,\n",
    "                                                        encoder_outputs, encoder_feature, enc_padding_mask, c_t_1,\n",
    "                                                        extra_zeros, enc_batch_extend_vocab,\n",
    "                                                                           coverage, di)\n",
    "            target = target_batch[:, di]\n",
    "            gold_probs = torch.gather(final_dist, 1, target.unsqueeze(1)).squeeze()\n",
    "            step_loss = -torch.log(gold_probs + g_eps)\n",
    "            if g_is_coverage:\n",
    "                step_coverage_loss = torch.sum(torch.min(attn_dist, coverage), 1)\n",
    "                step_loss = step_loss + g_cov_loss_wt * step_coverage_loss\n",
    "                coverage = next_coverage\n",
    "                \n",
    "            step_mask = dec_padding_mask[:, di]\n",
    "            step_loss = step_loss * step_mask\n",
    "            step_losses.append(step_loss)\n",
    "\n",
    "        sum_losses = torch.sum(torch.stack(step_losses, 1), 1)\n",
    "        batch_avg_loss = sum_losses/dec_lens_var\n",
    "        loss = torch.mean(batch_avg_loss)\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        self.norm = clip_grad_norm_(self.model.encoder.parameters(), g_max_grad_norm)\n",
    "        clip_grad_norm_(self.model.decoder.parameters(), g_max_grad_norm)\n",
    "        clip_grad_norm_(self.model.reduce_state.parameters(), g_max_grad_norm)\n",
    "\n",
    "        self.optimizer.step()\n",
    "\n",
    "        return loss.item()\n",
    "\n",
    "    def trainIters(self, n_iters, model_file_path=None):\n",
    "        iter, running_avg_loss = self.setup_train(model_file_path)\n",
    "        start = time.time()\n",
    "        while iter < n_iters:\n",
    "            batch = self.batcher.next_batch()\n",
    "            loss = self.train_one_batch(batch)\n",
    "\n",
    "            running_avg_loss = calc_running_avg_loss(loss, running_avg_loss, self.summary_writer, iter)\n",
    "            iter += 1\n",
    "\n",
    "            if iter % 100 == 0:\n",
    "                self.summary_writer.flush()\n",
    "            print_interval = 1\n",
    "            if iter % print_interval == 0:\n",
    "                print('steps %d, seconds for %d batch: %.2f , loss: %f' % (iter, print_interval,\n",
    "                                                                           time.time() - start, loss))\n",
    "                start = time.time()\n",
    "            if iter % 500 == 0:\n",
    "                self.save_model(running_avg_loss, iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output test set summary process definition\n",
    "class Beam(object):\n",
    "  def __init__(self, tokens, log_probs, state, context, coverage):\n",
    "    self.tokens = tokens\n",
    "    self.log_probs = log_probs\n",
    "    self.state = state\n",
    "    self.context = context\n",
    "    self.coverage = coverage\n",
    "\n",
    "  def extend(self, token, log_prob, state, context, coverage):\n",
    "    return Beam(tokens = self.tokens + [token],\n",
    "                      log_probs = self.log_probs + [log_prob],\n",
    "                      state = state,\n",
    "                      context = context,\n",
    "                      coverage = coverage)\n",
    "\n",
    "  @property\n",
    "  def latest_token(self):\n",
    "    return self.tokens[-1]\n",
    "\n",
    "  @property\n",
    "  def avg_log_prob(self):\n",
    "    return sum(self.log_probs) / len(self.tokens)\n",
    "\n",
    "\n",
    "class BeamSearch(object):\n",
    "    def __init__(self, model_file_path):\n",
    "        model_name = os.path.basename(model_file_path)\n",
    "        self._decode_dir = os.path.join(g_log_root, 'decode_%s' % (model_name))\n",
    "        self._rouge_ref_dir = os.path.join(self._decode_dir, 'rouge_ref')\n",
    "        self._rouge_dec_dir = os.path.join(self._decode_dir, 'rouge_dec_dir')\n",
    "        for p in [self._decode_dir, self._rouge_ref_dir, self._rouge_dec_dir]:\n",
    "            if not os.path.exists(p):\n",
    "                os.mkdir(p)\n",
    "\n",
    "        self.vocab = Vocab(g_vocab_path, g_vocab_size)\n",
    "        self.batcher = Batcher(g_decode_data_path, self.vocab, mode='decode',\n",
    "                               batch_size=g_beam_size, single_pass=True)\n",
    "        time.sleep(15)\n",
    "\n",
    "        self.model = Model(model_file_path, is_eval=True)\n",
    "\n",
    "    def sort_beams(self, beams):\n",
    "        return sorted(beams, key=lambda h: h.avg_log_prob, reverse=True)\n",
    "\n",
    "\n",
    "    def decode(self):\n",
    "        start = time.time()\n",
    "        counter = 0\n",
    "        batch = self.batcher.next_batch()\n",
    "        while batch is not None:\n",
    "            # Run beam search to get best Hypothesis\n",
    "            best_summary = self.beam_search(batch)\n",
    "\n",
    "            # Extract the output ids from the hypothesis and convert back to words\n",
    "            output_ids = [int(t) for t in best_summary.tokens[1:]]\n",
    "            decoded_words = outputids2words(output_ids, self.vocab,\n",
    "                                                 (batch.art_oovs[0] if g_pointer_gen else None))\n",
    "\n",
    "            # Remove the [STOP] token from decoded_words, if necessary\n",
    "            try:\n",
    "                fst_stop_idx = decoded_words.index(STOP_DECODING)\n",
    "                decoded_words = decoded_words[:fst_stop_idx]\n",
    "            except ValueError:\n",
    "                decoded_words = decoded_words\n",
    "                \n",
    "            original_abstract_sents = batch.original_abstracts_sents[0]\n",
    "\n",
    "            write_for_rouge(original_abstract_sents, decoded_words, counter,\n",
    "                            self._rouge_ref_dir, self._rouge_dec_dir)\n",
    "\n",
    "            counter += 1\n",
    "            if counter % 1000 == 0:\n",
    "                print('%d example in %d sec'%(counter, time.time() - start))\n",
    "                start = time.time()\n",
    "\n",
    "            batch = self.batcher.next_batch()\n",
    "            \n",
    "\n",
    "        print(\"Decoder has finished reading dataset for single_pass.\")\n",
    "\n",
    "\n",
    "    def beam_search(self, batch):\n",
    "        #batch should have only one example\n",
    "        enc_batch, enc_padding_mask, enc_lens, enc_batch_extend_vocab, extra_zeros, c_t_0, coverage_t_0 = \\\n",
    "            get_input_from_batch(batch, use_cuda)\n",
    "\n",
    "        encoder_outputs, encoder_feature, encoder_hidden = self.model.encoder(enc_batch, enc_lens)\n",
    "        s_t_0 = self.model.reduce_state(encoder_hidden)\n",
    "\n",
    "        dec_h, dec_c = s_t_0 # 1 x 2*hidden_size\n",
    "        dec_h = dec_h.squeeze()\n",
    "        dec_c = dec_c.squeeze()\n",
    "\n",
    "        #decoder batch preparation, it has beam_size example initially everything is repeated\n",
    "        beams = [Beam(tokens=[self.vocab.word2id(START_DECODING)],\n",
    "                      log_probs=[0.0],\n",
    "                      state=(dec_h[0], dec_c[0]),\n",
    "                      context = c_t_0[0],\n",
    "                      coverage=(coverage_t_0[0] if g_is_coverage else None))\n",
    "                 for _ in range(g_beam_size)]\n",
    "        results = []\n",
    "        steps = 0\n",
    "        while steps < g_max_dec_steps and len(results) < g_beam_size:\n",
    "            latest_tokens = [h.latest_token for h in beams]\n",
    "            latest_tokens = [t if t < self.vocab.size() else self.vocab.word2id(UNKNOWN_TOKEN) \\\n",
    "                             for t in latest_tokens]\n",
    "            y_t_1 = torch.LongTensor(latest_tokens)\n",
    "            if use_cuda:\n",
    "                y_t_1 = y_t_1.cuda()\n",
    "            all_state_h =[]\n",
    "            all_state_c = []\n",
    "\n",
    "            all_context = []\n",
    "\n",
    "            for h in beams:\n",
    "                state_h, state_c = h.state\n",
    "                all_state_h.append(state_h)\n",
    "                all_state_c.append(state_c)\n",
    "\n",
    "                all_context.append(h.context)\n",
    "\n",
    "            s_t_1 = (torch.stack(all_state_h, 0).unsqueeze(0), torch.stack(all_state_c, 0).unsqueeze(0))\n",
    "            c_t_1 = torch.stack(all_context, 0)\n",
    "\n",
    "            coverage_t_1 = None\n",
    "            if g_is_coverage:\n",
    "                all_coverage = []\n",
    "                for h in beams:\n",
    "                    all_coverage.append(h.coverage)\n",
    "                coverage_t_1 = torch.stack(all_coverage, 0)\n",
    "\n",
    "            final_dist, s_t, c_t, attn_dist, p_gen, coverage_t = self.model.decoder(y_t_1, s_t_1,\n",
    "                                                        encoder_outputs, encoder_feature, enc_padding_mask, c_t_1,\n",
    "                                                        extra_zeros, enc_batch_extend_vocab, coverage_t_1, steps)\n",
    "            log_probs = torch.log(final_dist)\n",
    "            topk_log_probs, topk_ids = torch.topk(log_probs, g_beam_size * 2)\n",
    "\n",
    "            dec_h, dec_c = s_t\n",
    "            dec_h = dec_h.squeeze()\n",
    "            dec_c = dec_c.squeeze()\n",
    "\n",
    "            all_beams = []\n",
    "            num_orig_beams = 1 if steps == 0 else len(beams)\n",
    "            for i in range(num_orig_beams):\n",
    "                h = beams[i]\n",
    "                state_i = (dec_h[i], dec_c[i])\n",
    "                context_i = c_t[i]\n",
    "                coverage_i = (coverage_t[i] if g_is_coverage else None)\n",
    "\n",
    "                for j in range(g_beam_size * 2):  # for each of the top 2*beam_size hyps:\n",
    "                    new_beam = h.extend(token=topk_ids[i, j].item(),\n",
    "                                   log_prob=topk_log_probs[i, j].item(),\n",
    "                                   state=state_i,\n",
    "                                   context=context_i,\n",
    "                                   coverage=coverage_i)\n",
    "                    all_beams.append(new_beam)\n",
    "\n",
    "            beams = []\n",
    "            for h in self.sort_beams(all_beams):\n",
    "                if h.latest_token == self.vocab.word2id(STOP_DECODING):\n",
    "                    if steps >= g_min_dec_steps:\n",
    "                        results.append(h)\n",
    "                else:\n",
    "                    beams.append(h)\n",
    "                if len(beams) == g_beam_size or len(results) == g_beam_size:\n",
    "                    break\n",
    "\n",
    "            steps += 1\n",
    "\n",
    "        if len(results) == 0:\n",
    "            results = beams\n",
    "\n",
    "        beams_sorted = self.sort_beams(results)\n",
    "\n",
    "        return beams_sorted[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: incorrectly formatted line in vocabulary file: 0800555111 356\n",
      "\n",
      "\n",
      "Warning: incorrectly formatted line in vocabulary file: 1800333000 139\n",
      "\n",
      "\n",
      "Warning: incorrectly formatted line in vocabulary file: 21/2 124\n",
      "\n",
      "\n",
      "Warning: incorrectly formatted line in vocabulary file: 31/2 86\n",
      "\n",
      "\n",
      "max_size of vocab was specified as 50000; we now have 50000 words. Stopping reading.\n",
      "Finished constructing vocabulary of 50000 total words. Last word added: perisic\n",
      "INFO:tensorflow:Bucket queue size: 0, Input queue size: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Bucket queue size: 0, Input queue size: 0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-29-b869d8a03696>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtrain_processor\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mtrain_processor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrainIters\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mg_max_iterations\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mg_model_file_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-23-de64c6410fa9>\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m      5\u001b[0m         self.batcher = Batcher(g_train_data_path, self.vocab, mode='train',\n\u001b[0;32m      6\u001b[0m                                batch_size=g_batch_size, single_pass=False)\n\u001b[1;32m----> 7\u001b[1;33m         \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m15\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m         \u001b[0mtrain_dir\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mg_log_root\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'train_%d'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Training the training set and output the model\n",
    "train_processor = Train()\n",
    "train_processor.trainIters(g_max_iterations, g_model_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: incorrectly formatted line in vocabulary file: 0800555111 356\n",
      "\n",
      "\n",
      "Warning: incorrectly formatted line in vocabulary file: 1800333000 139\n",
      "\n",
      "\n",
      "Warning: incorrectly formatted line in vocabulary file: 21/2 124\n",
      "\n",
      "\n",
      "Warning: incorrectly formatted line in vocabulary file: 31/2 86\n",
      "\n",
      "\n",
      "max_size of vocab was specified as 50000; we now have 50000 words. Stopping reading.\n",
      "Finished constructing vocabulary of 50000 total words. Last word added: perisic\n",
      "INFO:tensorflow:Bucket queue size: 100, Input queue size: 800\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Bucket queue size: 100, Input queue size: 800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Bucket queue size: 100, Input queue size: 800\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Bucket queue size: 100, Input queue size: 800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Bucket queue size: 100, Input queue size: 800\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Bucket queue size: 100, Input queue size: 800\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-65-def35a6cd995>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mde_model_filename\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'log/train_1619790107/model/model_30000_1619809438'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mbeam_Search_processor\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mBeamSearch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mde_model_filename\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mbeam_Search_processor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-63-1f242fe172f2>\u001b[0m in \u001b[0;36mdecode\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     51\u001b[0m         \u001b[1;32mwhile\u001b[0m \u001b[0mbatch\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     52\u001b[0m             \u001b[1;31m# Run beam search to get best Hypothesis\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 53\u001b[1;33m             \u001b[0mbest_summary\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbeam_search\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     54\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     55\u001b[0m             \u001b[1;31m# Extract the output ids from the hypothesis and convert back to words\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-63-1f242fe172f2>\u001b[0m in \u001b[0;36mbeam_search\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    134\u001b[0m                                                         \u001b[0mencoder_outputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoder_feature\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0menc_padding_mask\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mc_t_1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    135\u001b[0m                                                         extra_zeros, enc_batch_extend_vocab, coverage_t_1, steps)\n\u001b[1;32m--> 136\u001b[1;33m             \u001b[0mlog_probs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfinal_dist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    137\u001b[0m             \u001b[0mtopk_log_probs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtopk_ids\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtopk\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlog_probs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mg_beam_size\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    138\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# de_model_filename\n",
    "de_model_filename = 'log/train_1619790107/model/model_30000_1619809438'\n",
    "beam_Search_processor = BeamSearch(de_model_filename)\n",
    "beam_Search_processor.decode()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rouge\n",
    "def ROUGE(decode_path):\n",
    "    reference, sysout = [], []\n",
    "    de_path = Path(decode_path+'/rouge_dec_dir')\n",
    "    ref_path = Path(decode_path+'/rouge_ref')\n",
    "    for filename in de_path.iterdir():\n",
    "      fn = filename.stem\n",
    "      idx = fn[:6]\n",
    "      ref_n = f'{idx}_reference.txt'\n",
    "      with filename.open('r', encoding = 'utf-8') as de_f, (ref_path / ref_n).open('r', encoding = 'utf-8') as ref_f:\n",
    "        ref, de = [], []\n",
    "        for line in de_f:\n",
    "          de.append(line.strip())\n",
    "        for line in ref_f:\n",
    "          ref.append(line.strip())\n",
    "        reference.append([''.join(ref)])\n",
    "        sysout.append(''.join(de))\n",
    "\n",
    "    assert len(reference) == len(sysout), \"the size of reference and the size of sysout does't match\"\n",
    "    \n",
    "    rouge = Rouge()\n",
    "    score = {}\n",
    "      \n",
    "    for i in ['ROUGE-1', 'ROUGE-2', 'ROUGE-L']:\n",
    "      for j in ['r', 'p', 'f']:\n",
    "        score[f'{i}-{j}'] = []\n",
    "    for ref, syso in zip(reference, sysout):\n",
    "      s = rouge.get_scores(ref, [syso], avg=True)\n",
    "      for i in ['ROUGE-1', 'ROUGE-2', 'ROUGE-L']:\n",
    "        for j in ['r', 'p', 'f']:\n",
    "          score[f'{i}-{j}'].append(s[i.lower()][j])\n",
    "\n",
    "    for i in ['ROUGE-1', 'ROUGE-2', 'ROUGE-L']:\n",
    "        for j in ['r', 'p', 'f']:\n",
    "          score[f'{i}-{j}'] = np.mean(score[f'{i}-{j}'])\n",
    "    \n",
    "    print(score)\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ROUGE-1-r': 0.3754607329609624, 'ROUGE-1-p': 0.4145284911257925, 'ROUGE-1-f': 0.3801112638348548, 'ROUGE-2-r': 0.1639963385082581, 'ROUGE-2-p': 0.17978482110093358, 'ROUGE-2-f': 0.16520102701580966, 'ROUGE-L-r': 0.3667860044486288, 'ROUGE-L-p': 0.38637992127006937, 'ROUGE-L-f': 0.3662219087192046}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'ROUGE-1-r': 0.3754607329609624,\n",
       " 'ROUGE-1-p': 0.4145284911257925,\n",
       " 'ROUGE-1-f': 0.3801112638348548,\n",
       " 'ROUGE-2-r': 0.1639963385082581,\n",
       " 'ROUGE-2-p': 0.17978482110093358,\n",
       " 'ROUGE-2-f': 0.16520102701580966,\n",
       " 'ROUGE-L-r': 0.3667860044486288,\n",
       " 'ROUGE-L-p': 0.38637992127006937,\n",
       " 'ROUGE-L-f': 0.3662219087192046}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ROUGE('log\\decode_model_13500_1619809438')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
